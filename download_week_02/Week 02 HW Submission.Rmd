---
title: "Week 2 HW Submission"
author: "Spencer Swartz"
date: "January 31, 2017"
output: word_document
---

```{r include=FALSE, cache=FALSE}
# Don't modify this chunk of code, it is just installing and loading the DS705data package
if (!require(DS705data)){
  if (!require(devtools)){
    install.packages('devtools',repos="http://cran.rstudio.com/")
  }
  library(devtools)
  install_github('DataScienceUWL/DS705data')
}
require(DS705data)
# load the HealthExam data set into memory
data(HealthExam)
```

## Exercise 1

The following block of code will produce a simulated sampling distribution of the sample mean for 1,000 samples of size 23 drawn from an exponential distribution and then make a histogram of the results.  Some R programmers insist that for loops should be avoided because they are slow, but we're aiming for transparency at the moment, not efficiency.

```{r fig.width=3, fig.height=3}
# r defaults to a 7 by 7 figure (units?), use fig.width and fig.height to adjust
reps <- 1000
n <- 23
sampleStat <- numeric(reps) # initialize the vector
for (i in 1:reps){
  sampleStat[i] <- mean( rexp(n) )
}
hist(sampleStat)
```

You are going to demonstrate the Central Limit Theorem, and gain some practice with loops in R, by showing that distribution of the sample means becomes increasingly normal as the sample size increases.

### Part 1a

First, draw a random sample of 1000 observations from the exponential distribution and make a histogram to illustrate just how skewed the exponential distribution is.  You shouldn't need a for loop or mean() to do this bit.  (You're not taking means of anything ... )

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# create a histogram of 1000 random samples of the exponential distribution.
hist(rexp(1000))
```

----

### Part 1b

Copy the block of code at the top and use it to simulate the sampling distribution of sample means for 1000 samples of size 10.  After the histogram, use qqnorm() to make a normal probability plot of sampleStat.  Add a fit line to the plot with qqline().  Do the sample means appear to be normally distributed?  Explain. 


### -|-|-|-|-|-|-|-|-|-|-|- Answer 1b -|-|-|-|-|-|-|-|-|-|-|-
```{r}
# replace with your code
par(mfrow=c(1,2))
reps <- 1000
n <- 10
sampleStat <- numeric(reps) # initialize the vector
for (i in 1:reps){
  sampleStat[i] <- mean( rexp(n) )
}
hist(sampleStat)
qqnorm(sampleStat)
qqline(sampleStat)
#reset to 1 plot
par(mfrow=c(1,1))
```

It looks as if there may be some fat tails in this distrobution so it does not seem to be normaly distributed

----

### Part 1c

Repeat 1b for samples of size 200.  Do the sample means appear to closer to normally distributed than they were for n = 10?  Explain.


### -|-|-|-|-|-|-|-|-|-|-|- Answer 1c -|-|-|-|-|-|-|-|-|-|-|-
```{r}
# replace with your code.
par(mfrow=c(1,2))
reps <- 1000
n <- 200
sampleStat <- numeric(reps) # initialize the vector
for (i in 1:reps){
  sampleStat[i] <- mean( rexp(n) )
}
hist(sampleStat)
qqnorm(sampleStat)
qqline(sampleStat)
#reset to 1 plot
par(mfrow=c(1,1))
```

This looks to be more closely to a normal distrobution, the fat tails are not as pronounced

----

## Exercise 2

This problem is modeled loosely after problem 5.70 on page 287 of Ott.  

### Part 2a

Using the data $\bar{x} = 5.0, s = 0.7, n = 50$ we can determine that the 95% $t$-CI for $\mu$ is about (4.8,5.2) with margin of error 0.2.  For large samples $z \approx t$ and $\sigma \approx s$.  Use the formula on page 231 to estimate the sample size required to get margin of error $E \approx 0.05$.  Always round up for sample size.  Read Section 5.3 in Ott if you need to review this material.

###  -|-|-|-|-|-|-|-|-|-|-|- Answer 2a -|-|-|-|-|-|-|-|-|-|-|-
```{r}
# replace with your code
sig <- (5.2-4.8)/4
ceiling((((1.96)^2)*((sig)^2))/(.05^2))
```

---- 

### Part 2b

Suppose you now have a sample with size as determined in 2a that yields $\bar{x} = 5.03$ and $s = 0.68$  
Use R to build a fake data set with exactly the same statistics (as shown in the swirl lesson or consider the command scale() in R). The idea is to create a sample with exactly the right statistics so that we can use R functions to perform the analysis.  Now apply t.test to your constructed sample to find the 95% CI for the population mean.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2b -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# replace with your code
fakeData <- rnorm(50)
fakeData <- (fakeData - mean(fakeData))/sd(fakeData)
fakeData <- .68*fakeData + 5.03
print(mean(fakeData))
print(sd(fakeData))
t.test(fakeData, conf.level = .95)$conf.int
```

----

## Exercise 3

For this exercise and the rest of the exercises in this homework set you are going to use the data from problem 5.29 on page 279 of Ott.  This data is saved in the file 'ex5-29.TXT' that you downloaded along with this submission file.  You can load the data like this (assumes data file is the same directory as this file)

```{r echo}
# load csv into data frame
df <- read.csv('ex5-29.TXT')
# put data for lead concentrations in vector lead
lead <- df$Lead  
# delete the data frame since we no longer need it
rm(df)
```

### Part 3a

Before using a t distribution based procedure we need to determine if it is plausible that the data is sampled from a normally distributed random variable.  Make a histogram of the data.  Based on the histogram is it reasonable to say that lead concentrations are normally distributed?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# replace with your code
hist(lead)
```

No, it looks as if the data is not a normal distribution and it is skewed to the left.

----

### Part 3b
In spite of the fact that the lead concentration sample is clearly not from a mound-shaped distribution, we'll apply $t$ based procedures to start so we can compare them to a bootstrap approach a little later.
Use R to construct a 80% CI for the population mean lead concentration.  Write a sentence interpreting the result. (80% is a low confidence level, but for this problem were mostly interested in the lower bound so we're 90% confident that the pop mean is greater than the lower bound)

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3b -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# replace with your code
t.test(lead, conf.level = .80)$conf.int
```

There is an 80% chance that a given lead concentration level will be between 29.27 and 45.21.

---- 

### Part 3c
Note that your 80% CI for the population mean also gives you a 90% lower confidence bound for the population mean concentration.  Does this lower confidence bound suggest that the population mean lead concentration is larger than 30 mg/kg at the $\alpha = 0.10$ significance level?  Explain your answer (think of the equivalence of confidence intervals and hypothesis tests discussed in the class notes and NEWS anncouncement).

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3c -|-|-|-|-|-|-|-|-|-|-|-

No, this simpily means that there is a 90% chance that a given lead concentration level will be higher than 29.27.

----

### Part 3d

Use R to conduct a one-sample $t$-test to determine if the population mean lead concentration is larger than 30 mg/kg at the $\alpha = 0.1$ significance level.  Fill in all the steps in hypothesis test.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3d -|-|-|-|-|-|-|-|-|-|-|-

(Step 1) The hypotheses $H_0: \mu <= 30$ and $H_a: \mu > 30$  

(Step 2) Already did this in 3a.

(Step 3) Compute:  
```{r}
t.test(lead, mu = 30, alternative = "greater")
```

(Step 4) Conclusion:

We must accept the null Hypothesis (H0) as the p-value is greater than .1

### Part 3e

Since the lead concentrations are very skewed and the sample size is relatively small one should suspect the validity of the $t$ based procedures above.  The code below was used to bootstrap a CI for men's cholesterol levels in the notes.  Modify it produce a 80% CI for the population mean lead concentration using the lead data above.  You'll have to modify the data at the beginning, the confidence level, and change the degrees of freedom when plotting a true t distribution for comparison. Comment on differences between the theory and bootstrap intervals.  Is the bootstrap sampling distribution in good agreement with the t distribution from theory?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3e -|-|-|-|-|-|-|-|-|-|-|-

```{r, fig.width=5,fig.height=3}
chol <- lead
n <- length(chol)
xbar <- mean(chol)
s <- sd(chol)

reps <- 10000
t_sim <- numeric(reps)

# generate bootstrapped sampling distribution
for (i in 1:reps){
  x <- sample(chol,n,replace=TRUE)
  t_sim[i] <- (mean(x)-xbar)/(sd(x)/sqrt(n))
}

# build boostrapped "Studentized" t-inverval for mu
alpha <- .20
prbs <- c( 1-alpha/2, alpha/2 )

tcrit_est <- unname(quantile(t_sim, prob = prbs ))
t_ci_bootstrap <- xbar - tcrit_est*s/sqrt(n)
t_ci_bootstrap

# for comparison here is the t-interval from theory
t_ci_theory <- as.vector(t.test(chol)$conf.int)
t_ci_theory

# Compare the bootstrapped sampling distribution to the
# t-distribution from theory.  If the sample were from a normal distribution
# these would be in good agreement, instead we see some skewness in the smapling distribution ...
par(mar=(c(4,.5,0,.5)),yaxs="i",bty="l",xaxs="i",yaxt="n")
brks <- seq(min(t_sim),max(t_sim),length=50)
hist(t_sim,prob=TRUE,breaks=brks,main='',xlab='t',ylab='')
x0 <- seq(-4,4,length=400)
y0 <- dt(x0,df=n-1)
points(x0,y0,type='l',lwd=2,col='blue')
```


The low end of the boot strap does not quite fit with the low end in theory. We are not quite taking care of the fat tails.

### Part 3f

Using the bootstrapped sampling distribution approximate the $P$-value of the hypothesis test. Modify the code below that was used to approximate the $P$-value in the class notes.  How does it compare to the $P$-value from theory?  If testing at the 10% significance level, does your conclusion change? If so, give a new conclusion.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3f -|-|-|-|-|-|-|-|-|-|-|-

```{r,echo=FALSE, fig.width=5,fig.height=3}
chol <- lead
n <- length(chol)
xbar <- mean(chol)
s <- sd(chol)
mu0 <- 30
t0 <- (xbar-mu0)/(s/sqrt(n))
p0 = pt(t0,df = n-1, lower.tail=FALSE)

reps <- 10000
t_sim <- numeric(reps)

for (i in 1:reps){
  x <- sample(chol,n,replace=TRUE)
  t_sim[i] <- (mean(x)-xbar)/(sd(x)/sqrt(n))
}

p1 <- signif(sum(t_sim>t0)/reps,3)
p1

xpts <- seq(min(t_sim),max(t_sim),length=400)
ypts <- dt(xpts,df=n-1)
brks <- seq(min(t_sim),max(t_sim),length=50)
par(mar=(c(4,.5,0,.5)),yaxs="i",bty="l",xaxs="i",yaxt="n")
hist( t_sim, prob=TRUE, breaks=brks,main='',xlab='t',ylab='')
points(xpts,ypts,type = 'l',col='blue',lwd=2)
points(c(t0,t0),c(0,dt(t0,df=n-1)),type='l',col='red',lwd=2)
text(2.2,.2,bquote(P%~~%.(p1)))        
```

We can now reject the null Hypothesis (H0) as the p-value is less than .1

### Part 3g

Which do you trust more and why?  The results of the bootstrap procedures or the result of the theory based $t$ distribution procedures?  Explain.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3g -|-|-|-|-|-|-|-|-|-|-|-

Since the original data was very skewed the t-test was inaccurate. But since bootstraping accounts for the skewness we were able to reject H0, meaning the averge lead level is greater than 30.

----

### Part 3h

Suppose it would be worthwhile to be able to detect an alternative mean lead concentration of $\mu_a = 40 mg/kg$ when testing $H_a: \mu > 30$.  If using a sample of size 37 with an estimated standard deviation of 37.1 and $\alpha = 0.10$, use R to approximate the power of the $t$-test in this situation.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3h -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# your code goes here
power.t.test(n = n,delta = 10,sd = s, sig.level = .1, type = "one.sample", alternative = "one.sided")
```

----

### Part 3i

Suppose we need the power to be 0.9 to be able to detect an alternative mean lead concentration of $\mu_a = 40 mg/kg$ when testing $H_a: \mu > 30$.  Again, with estimated standard deviation of 37.1 and $\alpha = 0.10, what sample size is required for power = 0.9?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 3i -|-|-|-|-|-|-|-|-|-|-|-

```{r}
power.t.test(power = .9,delta = 10,sd = 37.1, sig.level = .1, type = "one.sample", alternative = "one.sided")$n
```

----



